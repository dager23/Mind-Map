{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6124b903",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3035d916",
   "metadata": {},
   "source": [
    "### Scrapping from web"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dc048afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "URL = 'https://en.wikipedia.org/wiki/Artificial_intelligence'\n",
    "html_page = requests.get(URL).text\n",
    "soup = BeautifulSoup(html_page, 'lxml')\n",
    "paraContent = soup.find_all('p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fbe7eeac",
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraph = \"\"\n",
    "for para in paraContent:\n",
    "    paragraph += para.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7a9d7069",
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraph = re.sub(r'\\[[0-9a-zA-Z]*\\]', ' ', paragraph)\n",
    "paragraph = re.sub(r'\\s+', ' ', paragraph)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e91eedfc",
   "metadata": {},
   "source": [
    "### NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eabadac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "daaeadfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_tokens = nltk.sent_tokenize(paragraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "05da2f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = nltk.corpus.stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7e01ec88",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_frequencies = {}\n",
    "word_tokens = nltk.word_tokenize(paragraph)\n",
    "for word in word_tokens:\n",
    "    if word not in stop_words:\n",
    "        if word not in word_frequencies.keys():\n",
    "            word_frequencies[word] = 1\n",
    "        else:\n",
    "            word_frequencies[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "89f8f395",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weighted Frequencies\n",
    "maximum_frquency_word = max(word_frequencies.values())\n",
    "for word in word_frequencies.keys():\n",
    "    word_frequencies[word] = (word_frequencies[word]/maximum_frquency_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "19038e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentence Score\n",
    "sentence_scores = {}\n",
    "for sentence in sentence_tokens:\n",
    "    for word in nltk.word_tokenize(sentence.lower()):\n",
    "        if word in word_frequencies.keys():\n",
    "            if(len(sentence.split(\" \")) < 30):\n",
    "                if sentence not in sentence_scores.keys():\n",
    "                    sentence_scores[sentence] = word_frequencies[word]\n",
    "                else:\n",
    "                    sentence_scores[sentence] += word_frequencies[word]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63fb4359",
   "metadata": {},
   "source": [
    "### Combine top sentences "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "707a17bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import heapq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7be72f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = heapq.nlargest(25, sentence_scores, key=sentence_scores.get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dd12a44f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = []\n",
    "for sentence in summary:\n",
    "    sentences.append(nltk.word_tokenize(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bdc34e2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Most EU member states had released national AI strategies, as had Canada, China, India, Japan, Mauritius, the Russian Federation, Saudi Arabia, United Arab Emirates, US and Vietnam. When given a small, static, and visible environment, this is easy; however, dynamic environments, such as (in endoscopy) the interior of a patient\\'s breathing body, pose a greater challenge. Other sectors included banking, entertainment, security, industry and manufacturing, agriculture, and networks (including social networks, smart cities and the Internet of things). The traditional goals of AI research include reasoning, knowledge representation, planning, learning, natural language processing, perception, and the ability to move and manipulate objects. It was followed, again in market size, by big data technologies, robotics, AI, 3D printing and the fifth generation of mobile services (5G). Soft computing is a set of techniques, including genetic algorithms, fuzzy logic and neural networks, that are tolerant of imprecision, uncertainty, partial truth and approximation. Formal knowledge representations are used in content-based indexing and retrieval, scene interpretation, clinical decision support, knowledge discovery (mining \"interesting\" and actionable inferences from large databases), and other areas. Many problems in AI (including in reasoning, planning, learning, perception, and robotics) require the agent to operate with incomplete or uncertain information. A simple \"neuron\" N accepts input from other neurons, each of which, when activated (or \"fired\"), casts a weighted \"vote\" for or against whether neuron N should itself activate. Soft computing tools were developed in the 1980s, such as neural networks, fuzzy systems, Grey system theory, evolutionary computation and many tools drawn from statistics or mathematical optimization. Robotics researchers, such as Rodney Brooks, rejected symbolic AI and focused on the basic engineering problems that would allow robots to move, survive, and learn their environment. AI also draws upon computer science, psychology, linguistics, philosophy, and many other fields. However, beginning with the collapse of the Lisp Machine market in 1987, AI once again fell into disrepute, and a second, longer-lasting winter began. Classifier performance depends greatly on the characteristics of the data to be classified, such as the dataset size, distribution of samples across classes, dimensionality, and the level of noise. A few examples are energy storage, deepfakes, medical diagnosis, military logistics, or supply chain management. \"Neats\" hope that intelligent behavior is described using simple, elegant principles (such as logic, optimization, or neural networks). AI founder John McCarthy agreed, writing that \"Artificial intelligence is not, by definition, simulation of human intelligence\". Deep learning has drastically improved the performance of programs in many important subfields of artificial intelligence, including computer vision, speech recognition, image classification and others. A superintelligence, hyperintelligence, or superhuman intelligence, is a hypothetical agent that would possess intelligence far surpassing that of the brightest and most gifted human mind. A representation of \"what exists\" is an ontology: the set of objects, relations, concepts, and properties formally described so that software agents can interpret them. The narrow focus allowed researchers to produce verifiable results, exploit more mathematical methods, and collaborate with other fields (such as statistics, economics and mathematics). Herbert Simon predicted, \"machines will be capable, within twenty years, of doing any work a man can do\". Specialized languages for artificial intelligence have been developed, such as Lisp, Prolog, TensorFlow and many others. Many researchers began to doubt that the symbolic approach would be able to imitate all the processes of human cognition, especially perception, robotics, learning and pattern recognition. Moderate successes related to affective computing include textual sentiment analysis and, more recently, multimodal sentiment analysis), wherein AI classifies the affects displayed by a videotaped subject.'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\" \".join(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06e0bafa",
   "metadata": {},
   "source": [
    "### Using sentence similarity and picking the best from the above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0f90c805",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.cluster.util import cosine_distance\n",
    "import numpy as np\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "027db199",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_similarity(sent1, sent2, stop_words):\n",
    "    sent1 = [w.lower() for w in sent1]\n",
    "    sent2 = [w.lower() for w in sent2]\n",
    "    all_words = list((set(sent1+sent2)))\n",
    "    \n",
    "    vector1 = [0] * len(all_words)\n",
    "    vector2 = [0] * len(all_words)\n",
    "    for w in sent1:\n",
    "        if w not in stop_words:\n",
    "            vector1[all_words.index(w)] += 1\n",
    "    for w in sent2:\n",
    "        if w not in stop_words:\n",
    "            vector2[all_words.index(w)] += 1\n",
    "    \n",
    "    return 1-cosine_distance(vector1, vector2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e57cc3bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_sim_matrix(sentences, stop_words):\n",
    "    similarity_matrix = np.zeros((len(sentences), len(sentences)))\n",
    "    for idx1 in range(len(sentences)):\n",
    "        for idx2 in range(len(sentences)):\n",
    "            if(idx1 == idx2):\n",
    "                continue\n",
    "            similarity_matrix[idx1][idx2] = sentence_similarity(sentences[idx1], sentences[idx2], stop_words)\n",
    "    \n",
    "    return similarity_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "16bc0be1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_summary(sentences, stop_words, top_n=5):\n",
    "    summarized_text = []\n",
    "    sentence_similarity_matrix = gen_sim_matrix(sentences, stop_words)\n",
    "    sentence_similarity_graph = nx.from_numpy_array(sentence_similarity_matrix)\n",
    "    scores = nx.pagerank(sentence_similarity_graph)\n",
    "    ranked_sentences = sorted(((scores[i], s) for i,s in enumerate(sentences)), reverse=True)\n",
    "    for i in range(top_n):\n",
    "        summarized_text.append(\" \".join(ranked_sentences[i][1]))\n",
    "    print(\"Summary: \\n\", \" \".join(summarized_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6fa44143",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary: \n",
      " Most EU member states had released national AI strategies , as had Canada , China , India , Japan , Mauritius , the Russian Federation , Saudi Arabia , United Arab Emirates , US and Vietnam . The traditional goals of AI research include reasoning , knowledge representation , planning , learning , natural language processing , perception , and the ability to move and manipulate objects . Other sectors included banking , entertainment , security , industry and manufacturing , agriculture , and networks ( including social networks , smart cities and the Internet of things ) . It was followed , again in market size , by big data technologies , robotics , AI , 3D printing and the fifth generation of mobile services ( 5G ) . When given a small , static , and visible environment , this is easy ; however , dynamic environments , such as ( in endoscopy ) the interior of a patient 's breathing body , pose a greater challenge . AI also draws upon computer science , psychology , linguistics , philosophy , and many other fields . Soft computing is a set of techniques , including genetic algorithms , fuzzy logic and neural networks , that are tolerant of imprecision , uncertainty , partial truth and approximation . Many problems in AI ( including in reasoning , planning , learning , perception , and robotics ) require the agent to operate with incomplete or uncertain information . A few examples are energy storage , deepfakes , medical diagnosis , military logistics , or supply chain management . However , beginning with the collapse of the Lisp Machine market in 1987 , AI once again fell into disrepute , and a second , longer-lasting winter began .\n"
     ]
    }
   ],
   "source": [
    "generate_summary(sentences, stop_words, top_n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4dc6250",
   "metadata": {},
   "source": [
    "## Combining all the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "95e5c79d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_data(URL):\n",
    "    html_page = requests.get(URL).text\n",
    "    soup = BeautifulSoup(html_page, 'lxml')\n",
    "    paraContent = soup.find_all('p')\n",
    "    paragraph = \"\"\n",
    "    for para in paraContent:\n",
    "        paragraph += para.text\n",
    "    paragraph = re.sub(r'\\[[0-9a-zA-Z]*\\]', ' ', paragraph)\n",
    "    paragraph = re.sub(r'\\s+', ' ', paragraph)\n",
    "    \n",
    "    return paragraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2d936c65",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_important_sentences(data):\n",
    "    sentence_tokens = nltk.sent_tokenize(data)\n",
    "    stop_words = nltk.corpus.stopwords.words('english')\n",
    "    word_frequencies = {}\n",
    "    word_tokens = nltk.word_tokenize(data)\n",
    "    for word in word_tokens:\n",
    "        if word not in stop_words:\n",
    "            if word not in word_frequencies.keys():\n",
    "                word_frequencies[word] = 1\n",
    "            else:\n",
    "                word_frequencies[word] += 1\n",
    "                \n",
    "    # Weighted Frequencies\n",
    "    maximum_frquency_word = max(word_frequencies.values())\n",
    "    for word in word_frequencies.keys():\n",
    "        word_frequencies[word] = (word_frequencies[word]/maximum_frquency_word)\n",
    "    \n",
    "    # Sentence Score\n",
    "    sentence_scores = {}\n",
    "    for sentence in sentence_tokens:\n",
    "        for word in nltk.word_tokenize(sentence.lower()):\n",
    "            if word in word_frequencies.keys():\n",
    "                if(len(sentence.split(\" \")) < 30):\n",
    "                    if sentence not in sentence_scores.keys():\n",
    "                        sentence_scores[sentence] = word_frequencies[word]\n",
    "                    else:\n",
    "                        sentence_scores[sentence] += word_frequencies[word]\n",
    "    \n",
    "    top_sentences = heapq.nlargest(25, sentence_scores, key=sentence_scores.get)\n",
    "    result = []\n",
    "    for sentence in top_sentences:\n",
    "        result.append(nltk.word_tokenize(sentence))\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3d2ea7bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_similarity(sent1, sent2, stop_words):\n",
    "    sent1 = [w.lower() for w in sent1]\n",
    "    sent2 = [w.lower() for w in sent2]\n",
    "    all_words = list((set(sent1+sent2)))\n",
    "    \n",
    "    vector1 = [0] * len(all_words)\n",
    "    vector2 = [0] * len(all_words)\n",
    "    for w in sent1:\n",
    "        if w not in stop_words:\n",
    "            vector1[all_words.index(w)] += 1\n",
    "    for w in sent2:\n",
    "        if w not in stop_words:\n",
    "            vector2[all_words.index(w)] += 1\n",
    "    \n",
    "    return 1-cosine_distance(vector1, vector2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "34601a31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_sim_matrix(sentences, stop_words):\n",
    "    similarity_matrix = np.zeros((len(sentences), len(sentences)))\n",
    "    for idx1 in range(len(sentences)):\n",
    "        for idx2 in range(len(sentences)):\n",
    "            if(idx1 == idx2):\n",
    "                continue\n",
    "            similarity_matrix[idx1][idx2] = sentence_similarity(sentences[idx1], sentences[idx2], stop_words)\n",
    "    \n",
    "    return similarity_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "08eb6823",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_summary(data, top_n=5):\n",
    "    sentences = get_important_sentences(data)\n",
    "    summarized_text = []\n",
    "    sentence_similarity_matrix = gen_sim_matrix(sentences, stop_words)\n",
    "    sentence_similarity_graph = nx.from_numpy_array(sentence_similarity_matrix)\n",
    "    scores = nx.pagerank(sentence_similarity_graph)\n",
    "    ranked_sentences = sorted(((scores[i], s) for i,s in enumerate(sentences)), reverse=True)\n",
    "    for i in range(top_n):\n",
    "        summarized_text.append(\" \".join(ranked_sentences[i][1]))\n",
    "    summary = \" \".join(summarized_text)\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "881e1f9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Such revolutions are usually recognized as having transformed in society , culture , philosophy , and technology much more than political systems ; they are often known as social revolutions . In his The Anatomy of Revolution , however , the Harvard historian Crane Brinton focused on the English Civil War , the American Revolution , the French Revolution , and the Russian Revolution . Katz also cross-classified revolutions as follows ; A further dimension to Katz 's typology is that revolutions are either against ( anti-monarchy , anti-dictatorial , anti-communist , anti-democratic ) or for ( pro-fascism , communism , nationalism etc . ) . Perhaps most often , the word `` revolution '' is employed to denote a change in social and political institutions . Political and socioeconomic revolutions have been studied in many social sciences , particularly sociology , political sciences and history . Scholars of revolutions , like Jack Goldstone , differentiate four current 'generations ' of scholarly research dealing with revolutions . Their results include major changes in culture , economy , and socio-political institutions , usually in response to perceived overwhelming autocracy or plutocracy . The works of Ted Robert Gurr , Ivo K. Feierbrand , Rosalind L. Feierbrand , James A. Geschwender , David C. Schwartz , and Denton E. Morrison fall into the first category . Revolutions have occurred throughout human history and vary widely in terms of methods , duration , and motivating ideology . The word `` revolucion '' is known in French from the 13th century , and `` revolution '' in English by the late fourteenth century , with regard to the revolving motion of celestial bodies .\n"
     ]
    }
   ],
   "source": [
    "scraped_data = scrape_data('https://en.wikipedia.org/wiki/Revolution')\n",
    "summary = generate_summary(scraped_data, top_n=10)\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2f44a11",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5752c1a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b8bcfb2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
